{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/udlbook/udlbook/blob/main/Notebooks/Chap12/12_1_Self_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","source":["# **Notebook 12.1: Self Attention**\n","\n","This notebook builds a self-attention mechanism from scratch, as discussed in section 12.2 of the book.\n","\n","Work through the cells below, running each cell in turn. In various places you will see the words \"TODO\". Follow the instructions at these places and make predictions about what is going to happen or write code to complete the functions.\n","\n","Contact me at udlbookmail@gmail.com if you find any mistakes or have any suggestions.\n","\n"],"metadata":{"id":"t9vk9Elugvmi"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt"],"metadata":{"id":"OLComQyvCIJ7","executionInfo":{"status":"ok","timestamp":1751609474123,"user_tz":-330,"elapsed":23,"user":{"displayName":"Yashas Nadig","userId":"14280923167211577465"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["The self-attention mechanism maps $N$ inputs $\\mathbf{x}_{n}\\in\\mathbb{R}^{D}$ and returns $N$ outputs $\\mathbf{x}'_{n}\\in \\mathbb{R}^{D}$.  \n","\n"],"metadata":{"id":"9OJkkoNqCVK2"}},{"cell_type":"code","source":["# Set seed so we get the same random numbers\n","np.random.seed(3)\n","# Number of inputs\n","N = 3\n","# Number of dimensions of each input\n","D = 4\n","# Create an empty list\n","all_x = []\n","# Create elements x_n and append to list\n","for n in range(N):\n","  x = np.random.normal(size=(D,1))\n","  all_x.append(x)\n","  print(f\"Input vector x_{n} (shape {x.shape}):\")\n","  print(f\"    {x.flatten()}\")\n","\n","print(f\"\\nOur complete input has {len(all_x)} vectors:\")\n"],"metadata":{"id":"oAygJwLiCSri","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751610929062,"user_tz":-330,"elapsed":7,"user":{"displayName":"Yashas Nadig","userId":"14280923167211577465"}},"outputId":"634218ce-008f-4d2f-ff7a-18649ec592dc"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Input vector x_0 (shape (4, 1)):\n","    [ 1.78862847  0.43650985  0.09649747 -1.8634927 ]\n","Input vector x_1 (shape (4, 1)):\n","    [-0.2773882  -0.35475898 -0.08274148 -0.62700068]\n","Input vector x_2 (shape (4, 1)):\n","    [-0.04381817 -0.47721803 -1.31386475  0.88462238]\n","\n","Our complete input has 3 vectors:\n"]}]},{"cell_type":"markdown","source":["We'll also need the weights and biases for the keys, queries, and values (equations 12.2 and 12.4)"],"metadata":{"id":"W2iHFbtKMaDp"}},{"cell_type":"code","source":["# Set seed so we get the same random numbers\n","np.random.seed(0)\n","\n","# Choose random values for the parameters\n","omega_q = np.random.normal(size=(D,D))\n","omega_k = np.random.normal(size=(D,D))\n","omega_v = np.random.normal(size=(D,D))\n","beta_q = np.random.normal(size=(D,1))\n","beta_k = np.random.normal(size=(D,1))\n","beta_v = np.random.normal(size=(D,1))"],"metadata":{"id":"79TSK7oLMobe","executionInfo":{"status":"ok","timestamp":1751610955897,"user_tz":-330,"elapsed":12,"user":{"displayName":"Yashas Nadig","userId":"14280923167211577465"}}},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":["Now let's compute the queries, keys, and values for each input"],"metadata":{"id":"VxaKQtP3Ng6R"}},{"cell_type":"code","source":["# Make three lists to store queries, keys, and values\n","all_queries = []\n","all_keys = []\n","all_values = []\n","\n","# For every input\n","for i, x in enumerate(all_x):\n","  print(f\"\\nTransforming input x_{i}:\")\n","  print(f\"    Original x_{i}: {x.flatten()}\")\n","\n","  # Compute the keys, queries and values\n","  query = beta_q + omega_q @ x\n","  key = beta_k + omega_k @ x\n","  value = beta_v + omega_v @ x\n","\n","  print(f\"        {beta_q.shape} + {omega_q.shape} * {x.shape}\")\n","  print(f\"       Query_{i} = β_q + Ω_q @ x_{i}\")\n","  print(f\"        Shape: {query.shape}, Values: {query.flatten()}\")\n","  print(f\"       Key_{i} = β_k + Ω_k @ x_{i}\")\n","  print(f\"        Shape: {key.shape}, Values: {key.flatten()}\")\n","  print(f\"       Value_{i} = β_v + Ω_v @ x_{i}\")\n","  print(f\"        Shape: {value.shape}, Values: {value.flatten()}\")\n","\n","  all_queries.append(query)\n","  all_keys.append(key)\n","  all_values.append(value)\n","\n","print(f\"\\n  Summary after transformation:\")\n","print(f\"    We now have {len(all_queries)} queries, {len(all_keys)} keys, and {len(all_values)} values\")\n","print(f\"    Each query/key/value has shape: {all_queries[0].shape}\")"],"metadata":{"id":"TwDK2tfdNmw9","executionInfo":{"status":"ok","timestamp":1751611180186,"user_tz":-330,"elapsed":10,"user":{"displayName":"Yashas Nadig","userId":"14280923167211577465"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e234f2b8-cd71-40ed-a528-eb33e610ed7f"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Transforming input x_0:\n","    Original x_0: [ 1.78862847  0.43650985  0.09649747 -1.8634927 ]\n","        (4, 1) + (4, 4) * (4, 1)\n","       Query_0 = β_q + Ω_q @ x_0\n","        Shape: (4, 1), Values: [-2.36543342  3.07476988 -3.59698468  1.22226059]\n","       Key_0 = β_k + Ω_k @ x_0\n","        Shape: (4, 1), Values: [ 3.69380505 -3.9952365   3.7499519   3.12154293]\n","       Value_0 = β_v + Ω_v @ x_0\n","        Shape: (4, 1), Values: [-2.71096662  3.5538184  -6.92955207 -3.0352825 ]\n","\n","Transforming input x_1:\n","    Original x_1: [-0.2773882  -0.35475898 -0.08274148 -0.62700068]\n","        (4, 1) + (4, 4) * (4, 1)\n","       Query_1 = β_q + Ω_q @ x_1\n","        Shape: (4, 1), Values: [-3.7312083  -0.36779138 -1.93624723 -0.11330562]\n","       Key_1 = β_k + Ω_k @ x_1\n","        Shape: (4, 1), Values: [-0.34284839 -0.31052676 -0.02825783 -0.76803995]\n","       Value_1 = β_v + Ω_v @ x_1\n","        Shape: (4, 1), Values: [ 0.94623971 -0.24375925 -0.92165993 -0.44978771]\n","\n","Transforming input x_2:\n","    Original x_2: [-0.04381817 -0.47721803 -1.31386475  0.88462238]\n","        (4, 1) + (4, 4) * (4, 1)\n","       Query_2 = β_q + Ω_q @ x_2\n","        Shape: (4, 1), Values: [-1.18574269 -1.21038029  0.01034041  0.00748923]\n","       Key_2 = β_k + Ω_k @ x_2\n","        Shape: (4, 1), Values: [-1.64524855 -3.17297146  0.34070328 -0.20908514]\n","       Value_2 = β_v + Ω_v @ x_2\n","        Shape: (4, 1), Values: [ 1.64610946 -0.08376326  4.05678958  2.20243685]\n","\n","  Summary after transformation:\n","    We now have 3 queries, 3 keys, and 3 values\n","    Each query/key/value has shape: (4, 1)\n"]}]},{"cell_type":"markdown","source":["We'll need a softmax function (equation 12.5) -- here, it will take a list of arbitrary numbers and return a list where the elements are non-negative and sum to one\n"],"metadata":{"id":"Se7DK6PGPSUk"}},{"cell_type":"code","source":["def softmax(items_in):\n","  print(f\"       Softmax input: {items_in}\")\n","  e_x = np.exp(items_in - np.max(items_in))\n","  items_out = e_x / e_x.sum()\n","  print(f\"       Softmax output: {items_out} (sum = {items_out.sum():.6f})\")\n","  return items_out"],"metadata":{"id":"u93LIcE5PoiM","executionInfo":{"status":"ok","timestamp":1751611220482,"user_tz":-330,"elapsed":5,"user":{"displayName":"Yashas Nadig","userId":"14280923167211577465"}}},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":["Now compute the self attention values:"],"metadata":{"id":"8aJVhbKDW7lm"}},{"cell_type":"code","source":["# Create empty list for output\n","all_x_prime = []\n","\n","# For each output\n","for n in range(N):\n","  print(f\"\\n  Computing output x'_{n} (the {n}-th attended representation):\")\n","  print(f\"    We'll see how much x'_{n} should 'pay attention' to each input\")\n","\n","  # Create list for dot products of query N with all keys\n","  all_km_qn = []\n","  print(f\"\\n       Step 1: Query_{n} meets all Keys (computing compatibility)\")\n","\n","  # Compute the dot products\n","  for m, key in enumerate(all_keys):\n","    print(f\"        Query_{n} • Key_{m}:\")\n","    print(f\"            Query_{n}: {all_queries[n].flatten()}\")\n","    print(f\"            Key_{m}: {key.flatten()}\")\n","\n","    dot_product = key.T @ all_queries[n]\n","    scalar_dot = dot_product[0,0]  # Extract scalar from 1x1 matrix\n","\n","    print(f\"            Dot product: {scalar_dot:.6f}\")\n","    print(f\"            (This measures how 'compatible' query_{n} is with key_{m})\")\n","\n","    all_km_qn.append(scalar_dot)\n","\n","  # Convert the list of dot products to a numpy array\n","  all_km_qn = np.array(all_km_qn)\n","  print(f\"\\n      All compatibility scores for output {n}: {all_km_qn}\")\n","\n","  # Compute attention\n","  print(f\"\\n      Step 2: Converting compatibility to attention weights (softmax)\")\n","  attention = softmax(all_km_qn)\n","\n","  print(f\"       Final attention weights for output {n}: {attention}\")\n","  print(f\"       Interpretation:\")\n","  for m in range(N):\n","    percentage = attention[m] * 100\n","    print(f\"        Output {n} pays {percentage:.2f}% attention to input {m}\")\n","\n","  print(f\"\\n       Step 3: Creating the weighted combination of values\")\n","  x_prime = np.zeros((D,1))\n","  print(f\"        Starting with zero vector: {x_prime.flatten()}\")\n","\n","  for m in range(N):\n","    contribution = attention[m] * all_values[m]\n","    print(f\"        + {attention[m]:.6f} × Value_{m}\")\n","    print(f\"          = {attention[m]:.6f} × {all_values[m].flatten()}\")\n","    print(f\"          = {contribution.flatten()}\")\n","    x_prime += contribution\n","    print(f\"        Running sum: {x_prime.flatten()}\")\n","\n","  print(f\"\\n       Final result x'_{n}: {x_prime.flatten()}\")\n","  all_x_prime.append(x_prime)\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"  The Final Results\")\n","print(\"=\"*80)\n","\n","# Print out true values to check you have it correct\n","print(\"x_prime_0_calculated:\", all_x_prime[0].transpose())\n","print(\"x_prime_0_true: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\")\n","print(\"x_prime_1_calculated:\", all_x_prime[1].transpose())\n","print(\"x_prime_1_true: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\")\n","print(\"x_prime_2_calculated:\", all_x_prime[2].transpose())\n","print(\"x_prime_2_true: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\")"],"metadata":{"id":"yimz-5nCW6vQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751611476549,"user_tz":-330,"elapsed":48,"user":{"displayName":"Yashas Nadig","userId":"14280923167211577465"}},"outputId":"fdfccd1c-4345-45b3-db03-ea94d690e901"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","  Computing output x'_0 (the 0-th attended representation):\n","    We'll see how much x'_0 should 'pay attention' to each input\n","\n","       Step 1: Query_0 meets all Keys (computing compatibility)\n","        Query_0 • Key_0:\n","            Query_0: [-2.36543342  3.07476988 -3.59698468  1.22226059]\n","            Key_0: [ 3.69380505 -3.9952365   3.7499519   3.12154293]\n","            Dot product: -30.695063\n","            (This measures how 'compatible' query_0 is with key_0)\n","        Query_0 • Key_1:\n","            Query_0: [-2.36543342  3.07476988 -3.59698468  1.22226059]\n","            Key_1: [-0.34284839 -0.31052676 -0.02825783 -0.76803995]\n","            Dot product: -0.980915\n","            (This measures how 'compatible' query_0 is with key_1)\n","        Query_0 • Key_2:\n","            Query_0: [-2.36543342  3.07476988 -3.59698468  1.22226059]\n","            Key_2: [-1.64524855 -3.17297146  0.34070328 -0.20908514]\n","            Dot product: -7.345492\n","            (This measures how 'compatible' query_0 is with key_2)\n","\n","      All compatibility scores for output 0: [-30.69506335  -0.98091527  -7.34549217]\n","\n","      Step 2: Converting compatibility to attention weights (softmax)\n","       Softmax input: [-30.69506335  -0.98091527  -7.34549217]\n","       Softmax output: [1.24326146e-13 9.98281489e-01 1.71851130e-03] (sum = 1.000000)\n","       Final attention weights for output 0: [1.24326146e-13 9.98281489e-01 1.71851130e-03]\n","       Interpretation:\n","        Output 0 pays 0.00% attention to input 0\n","        Output 0 pays 99.83% attention to input 1\n","        Output 0 pays 0.17% attention to input 2\n","\n","       Step 3: Creating the weighted combination of values\n","        Starting with zero vector: [0. 0. 0. 0.]\n","        + 0.000000 × Value_0\n","          = 0.000000 × [-2.71096662  3.5538184  -6.92955207 -3.0352825 ]\n","          = [-3.37044032e-13  4.41832546e-13 -8.61524503e-13 -3.77364976e-13]\n","        Running sum: [-3.37044032e-13  4.41832546e-13 -8.61524503e-13 -3.77364976e-13]\n","        + 0.998281 × Value_1\n","          = 0.998281 × [ 0.94623971 -0.24375925 -0.92165993 -0.44978771]\n","          = [ 0.94461358 -0.24334034 -0.92007604 -0.44901474]\n","        Running sum: [ 0.94461358 -0.24334034 -0.92007604 -0.44901474]\n","        + 0.001719 × Value_2\n","          = 0.001719 × [ 1.64610946 -0.08376326  4.05678958  2.20243685]\n","          = [ 0.00282886 -0.00014395  0.00697164  0.00378491]\n","        Running sum: [ 0.94744244 -0.24348429 -0.91310441 -0.44522983]\n","\n","       Final result x'_0: [ 0.94744244 -0.24348429 -0.91310441 -0.44522983]\n","\n","  Computing output x'_1 (the 1-th attended representation):\n","    We'll see how much x'_1 should 'pay attention' to each input\n","\n","       Step 1: Query_1 meets all Keys (computing compatibility)\n","        Query_1 • Key_0:\n","            Query_1: [-3.7312083  -0.36779138 -1.93624723 -0.11330562]\n","            Key_0: [ 3.69380505 -3.9952365   3.7499519   3.12154293]\n","            Dot product: -19.927465\n","            (This measures how 'compatible' query_1 is with key_0)\n","        Query_1 • Key_1:\n","            Query_1: [-3.7312083  -0.36779138 -1.93624723 -0.11330562]\n","            Key_1: [-0.34284839 -0.31052676 -0.02825783 -0.76803995]\n","            Dot product: 1.535185\n","            (This measures how 'compatible' query_1 is with key_1)\n","        Query_1 • Key_2:\n","            Query_1: [-3.7312083  -0.36779138 -1.93624723 -0.11330562]\n","            Key_2: [-1.64524855 -3.17297146  0.34070328 -0.20908514]\n","            Dot product: 6.669761\n","            (This measures how 'compatible' query_1 is with key_2)\n","\n","      All compatibility scores for output 1: [-19.92746485   1.53518521   6.66976135]\n","\n","      Step 2: Converting compatibility to attention weights (softmax)\n","       Softmax input: [-19.92746485   1.53518521   6.66976135]\n","       Softmax output: [2.79525306e-12 5.85506360e-03 9.94144936e-01] (sum = 1.000000)\n","       Final attention weights for output 1: [2.79525306e-12 5.85506360e-03 9.94144936e-01]\n","       Interpretation:\n","        Output 1 pays 0.00% attention to input 0\n","        Output 1 pays 0.59% attention to input 1\n","        Output 1 pays 99.41% attention to input 2\n","\n","       Step 3: Creating the weighted combination of values\n","        Starting with zero vector: [0. 0. 0. 0.]\n","        + 0.000000 × Value_0\n","          = 0.000000 × [-2.71096662  3.5538184  -6.92955207 -3.0352825 ]\n","          = [-7.57783774e-12  9.93382177e-12 -1.93698516e-11 -8.48438271e-12]\n","        Running sum: [-7.57783774e-12  9.93382177e-12 -1.93698516e-11 -8.48438271e-12]\n","        + 0.005855 × Value_1\n","          = 0.005855 × [ 0.94623971 -0.24375925 -0.92165993 -0.44978771]\n","          = [ 0.00554029 -0.00142723 -0.00539638 -0.00263354]\n","        Running sum: [ 0.00554029 -0.00142723 -0.00539638 -0.00263354]\n","        + 0.994145 × Value_2\n","          = 0.994145 × [ 1.64610946 -0.08376326  4.05678958  2.20243685]\n","          = [ 1.63647139 -0.08327282  4.03303682  2.18954144]\n","        Running sum: [ 1.64201168 -0.08470004  4.02764044  2.18690791]\n","\n","       Final result x'_1: [ 1.64201168 -0.08470004  4.02764044  2.18690791]\n","\n","  Computing output x'_2 (the 2-th attended representation):\n","    We'll see how much x'_2 should 'pay attention' to each input\n","\n","       Step 1: Query_2 meets all Keys (computing compatibility)\n","        Query_2 • Key_0:\n","            Query_2: [-1.18574269 -1.21038029  0.01034041  0.00748923]\n","            Key_0: [ 3.69380505 -3.9952365   3.7499519   3.12154293]\n","            Dot product: 0.518007\n","            (This measures how 'compatible' query_2 is with key_0)\n","        Query_2 • Key_1:\n","            Query_2: [-1.18574269 -1.21038029  0.01034041  0.00748923]\n","            Key_1: [-0.34284839 -0.31052676 -0.02825783 -0.76803995]\n","            Dot product: 0.776341\n","            (This measures how 'compatible' query_2 is with key_1)\n","        Query_2 • Key_2:\n","            Query_2: [-1.18574269 -1.21038029  0.01034041  0.00748923]\n","            Key_2: [-1.64524855 -3.17297146  0.34070328 -0.20908514]\n","            Dot product: 5.793301\n","            (This measures how 'compatible' query_2 is with key_2)\n","\n","      All compatibility scores for output 2: [0.51800715 0.77634121 5.79330067]\n","\n","      Step 2: Converting compatibility to attention weights (softmax)\n","       Softmax input: [0.51800715 0.77634121 5.79330067]\n","       Softmax output: [0.00505708 0.00654776 0.98839516] (sum = 1.000000)\n","       Final attention weights for output 2: [0.00505708 0.00654776 0.98839516]\n","       Interpretation:\n","        Output 2 pays 0.51% attention to input 0\n","        Output 2 pays 0.65% attention to input 1\n","        Output 2 pays 98.84% attention to input 2\n","\n","       Step 3: Creating the weighted combination of values\n","        Starting with zero vector: [0. 0. 0. 0.]\n","        + 0.005057 × Value_0\n","          = 0.005057 × [-2.71096662  3.5538184  -6.92955207 -3.0352825 ]\n","          = [-0.01370957  0.01797194 -0.03504329 -0.01534966]\n","        Running sum: [-0.01370957  0.01797194 -0.03504329 -0.01534966]\n","        + 0.006548 × Value_1\n","          = 0.006548 × [ 0.94623971 -0.24375925 -0.92165993 -0.44978771]\n","          = [ 0.00619575 -0.00159608 -0.00603481 -0.0029451 ]\n","        Running sum: [-0.00751382  0.01637586 -0.0410781  -0.01829477]\n","        + 0.988395 × Value_2\n","          = 0.988395 × [ 1.64610946 -0.08376326  4.05678958  2.20243685]\n","          = [ 1.62700663 -0.0827912   4.00971118  2.17687792]\n","        Running sum: [ 1.61949281 -0.06641533  3.96863308  2.15858316]\n","\n","       Final result x'_2: [ 1.61949281 -0.06641533  3.96863308  2.15858316]\n","\n","================================================================================\n","  The Final Results\n","================================================================================\n","x_prime_0_calculated: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\n","x_prime_0_true: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\n","x_prime_1_calculated: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\n","x_prime_1_true: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\n","x_prime_2_calculated: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\n","x_prime_2_true: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\n"]}]},{"cell_type":"markdown","source":["Now let's compute the same thing, but using matrix calculations.  We'll store the $N$ inputs $\\mathbf{x}_{n}\\in\\mathbb{R}^{D}$ in the columns of a $D\\times N$ matrix, using equations 12.6 and 12.7/8.\n","\n","Note:  The book uses column vectors (for compatibility with the rest of the text), but in the wider literature it is more normal to store the inputs in the rows of a matrix;  in this case, the computation is the same, but all the matrices are transposed and the operations proceed in the reverse order."],"metadata":{"id":"PJ2vCQ_7C38K"}},{"cell_type":"code","source":["# Define softmax operation that works independently on each column\n","def softmax_cols(data_in):\n","  # Exponentiate all of the values\n","  exp_values = np.exp(data_in) ;\n","  # Sum over columns\n","  denom = np.sum(exp_values, axis = 0);\n","  # Replicate denominator to N rows\n","  denom = np.matmul(np.ones((data_in.shape[0],1)), denom[np.newaxis,:])\n","  # Compute softmax\n","  softmax = exp_values / denom\n","  # return the answer\n","  return softmax"],"metadata":{"id":"obaQBdUAMXXv","executionInfo":{"status":"ok","timestamp":1751611532185,"user_tz":-330,"elapsed":16,"user":{"displayName":"Yashas Nadig","userId":"14280923167211577465"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["# Now let's compute self attention in matrix form\n","def self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k):\n","\n","  # TODO -- Write this function\n","  # 1. Compute queries, keys, and values\n","  # 2. Compute dot products\n","  # 3. Apply softmax to calculate attentions\n","  # 4. Weight values by attentions\n","  # Replace this line\n","\n","  # Queries, Keys and Values\n","  Q = beta_q @ np.ones((1,N)) + omega_q @ X\n","  K = beta_k @ np.ones((1,N)) + omega_k @ X\n","  V = beta_v @ np.ones((1,N)) + omega_v @ X\n","\n","  # Dot product\n","  dot_product = K.T @ Q\n","\n","  # Apply softmax\n","  attention = softmax_cols(dot_product)\n","\n","  # Weight values\n","  X_prime = V @ attention\n","\n","  return X_prime"],"metadata":{"id":"gb2WvQ3SiH8r","executionInfo":{"status":"ok","timestamp":1751611989023,"user_tz":-330,"elapsed":6,"user":{"displayName":"Yashas Nadig","userId":"14280923167211577465"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["# Copy data into matrix\n","X = np.zeros((D, N))\n","X[:,0] = np.squeeze(all_x[0])\n","X[:,1] = np.squeeze(all_x[1])\n","X[:,2] = np.squeeze(all_x[2])\n","\n","# Run the self attention mechanism\n","X_prime = self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n","\n","# Print out the results\n","print(X_prime)"],"metadata":{"id":"MUOJbgJskUpl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751611992155,"user_tz":-330,"elapsed":8,"user":{"displayName":"Yashas Nadig","userId":"14280923167211577465"}},"outputId":"2b98568e-abf0-4050-82fe-1a6c9696e777"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0.94744244  1.64201168  1.61949281]\n"," [-0.24348429 -0.08470004 -0.06641533]\n"," [-0.91310441  4.02764044  3.96863308]\n"," [-0.44522983  2.18690791  2.15858316]]\n"]}]},{"cell_type":"markdown","source":["If you did this correctly, the values should be the same as above.\n","\n","TODO:  \n","\n","Print out the attention matrix\n","You will see that the values are quite extreme (one is very close to one and the others are very close to zero.  Now we'll fix this problem by using scaled dot-product attention."],"metadata":{"id":"as_lRKQFpvz0"}},{"cell_type":"code","source":["print(attention)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"noNlmlnwM9Qr","executionInfo":{"status":"ok","timestamp":1751611869548,"user_tz":-330,"elapsed":24,"user":{"displayName":"Yashas Nadig","userId":"14280923167211577465"}},"outputId":"ef0ff83c-aa2f-4d28-ccc8-ed9eecaa8390"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.00505708 0.00654776 0.98839516]\n"]}]},{"cell_type":"code","source":["# Now let's compute self attention in matrix form\n","def scaled_dot_product_self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k):\n","\n","  # TODO -- Write this function\n","  # 1. Compute queries, keys, and values\n","  # 2. Compute dot products\n","  # 3. Scale the dot products as in equation 12.9\n","  # 4. Apply softmax to calculate attentions\n","  # 5. Weight values by attentions\n","  # Replace this line\n","\n","  # Compute Queries, Keys and Values\n","  Q = beta_q @ np.ones((1, N)) + omega_q @ X\n","  K = beta_k @ np.ones((1, N)) + omega_k @ X\n","  V = beta_v @ np.ones((1, N)) + omega_v @ X\n","\n","  # Compute dot product\n","  dot_product = K.T @ Q\n","\n","  # Scale the dot product\n","  scaled_dot_product = dot_product / np.sqrt(omega_q.shape[0])\n","\n","  # Apply softmax to calculate attentions\n","  attention = softmax_cols(scaled_dot_product)\n","\n","  # Multiply Value matrix with attention\n","  x_prime = V @ attention\n","\n","  return X_prime"],"metadata":{"id":"kLU7PUnnqvIh","executionInfo":{"status":"ok","timestamp":1751612265454,"user_tz":-330,"elapsed":149,"user":{"displayName":"Yashas Nadig","userId":"14280923167211577465"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["# Run the self attention mechanism\n","X_prime = scaled_dot_product_self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n","\n","# Print out the results\n","print(X_prime)"],"metadata":{"id":"n18e3XNzmVgL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751612270432,"user_tz":-330,"elapsed":7,"user":{"displayName":"Yashas Nadig","userId":"14280923167211577465"}},"outputId":"e92195db-94ca-459d-b396-ddb7a07528b6"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0.94744244  1.64201168  1.61949281]\n"," [-0.24348429 -0.08470004 -0.06641533]\n"," [-0.91310441  4.02764044  3.96863308]\n"," [-0.44522983  2.18690791  2.15858316]]\n"]}]},{"cell_type":"markdown","source":["TODO -- Investigate whether the self-attention mechanism is covariant with respect to permutation.\n","If it is, when we permute the columns of the input matrix $\\mathbf{X}$, the columns of the output matrix $\\mathbf{X}'$ will also be permuted.\n"],"metadata":{"id":"QDEkIrcgrql-"}}]}